{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33ee94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install rouge_score\n",
    "#!pip install nltk\n",
    "#!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0840e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 16:13:17.397468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModel,\n",
    "    RobertaModel, \n",
    "    RobertaTokenizer\n",
    ")\n",
    "from tabulate import tabulate\n",
    "import model\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from data_utils import to_cuda, collate_mp, ReRankingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from compare_mt.rouge.rouge_scorer import RougeScorer\n",
    "import time\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb0652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeafea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10521b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "# change to args parser if use py file\n",
    "class my_args:\n",
    "    def __init__(self,generator_name, pt_scorer_name, csv_data_name, decoder_max_len, num_cands, sum_max_len,cand_gen_batch):\n",
    "        self.generator_name = generator_name\n",
    "        self.csv_data_name = csv_data_name\n",
    "        self.decoder_max_len = decoder_max_len # default to 30 since title are short\n",
    "        self.num_cands = num_cands\n",
    "        self.sum_max_len = sum_max_len\n",
    "        self.pt_scorer_name = pt_scorer_name\n",
    "args = my_args('tuned_t5_model', 'cache/scorer_5.bin', 'clean_covid.csv',30, 8, 30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955ba7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3842948189a04584b4456d4432be4dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = args.generator_name\n",
    "generator = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encoder_max_length = 512 # default to 512\n",
    "decoder_max_length = args.decoder_max_len\n",
    "\n",
    "# read_dataset preprocess data\n",
    "csv_data_name = pd.read_csv(args.csv_data_name)\n",
    "data = csv_data_name[['abstract','title']].dropna()\n",
    "dataset = Dataset.from_pandas(data)\n",
    "train_data_txt, remain_data_txt = dataset.train_test_split(test_size=0.2).values()\n",
    "val_data_txt, test_data_txt = remain_data_txt.train_test_split(test_size=0.5).values()\n",
    "train_data_txt = train_data_txt.shuffle(seed = 2333).select(range(int(len(train_data_txt)/10)))\n",
    "val_data_txt = val_data_txt.shuffle(seed = 2333).select(range(int(len(val_data_txt)/10)))\n",
    "test_data_txt = test_data_txt.shuffle(seed = 2333).select(range(int(len(test_data_txt)/10)))\n",
    "# tokenize data\n",
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source, target = batch[\"abstract\"], batch[\"title\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == generator_tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "test_data = test_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, generator_tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=test_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e30866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8148\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96110811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a522d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(ids):\n",
    "    return torch.tensor(ids, dtype=torch.long).to(device)\n",
    "def show_a_piece_of_data(generator,generator_tokenizer,sample):\n",
    "    test_article = sample['abstract']\n",
    "    reference = sample['title']\n",
    "    num_return_seqs=16\n",
    "    input_ids = torch.tensor(generator_tokenizer(test_article)['input_ids'], dtype=torch.long).to(device)\n",
    "    train_output_ids = generator.generate(input_ids,num_beams = num_return_seqs,\n",
    "                                     #no_repeat_ngram_size=2,\n",
    "                                     diversity_penalty=1.0,\n",
    "                                     max_length = 20,\n",
    "                                     num_beam_groups = num_return_seqs,\n",
    "                                     num_return_sequences=num_return_seqs)\n",
    "    cands = generator_tokenizer.batch_decode(train_output_ids, skip_special_tokens=True)\n",
    "    print(cands)\n",
    "    print('ref'+'-'*20)\n",
    "    print(reference)\n",
    "    print('doc'+'-'*20)\n",
    "    print(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b814a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the university of british columbia undergraduate program mdup students and', 'impact of the covid pandemic on medical education the university of british col', 'collaboration between university of british columbia undergraduate students and faculty during the co', 'a joint response to covid disruptions in medical education', 'the covid pandemic and the medical student response team in the university of brit', 'university of british columbia undergraduate program students and faculty a joint response', 'a joint response to covid disruptions in medical education a collaboration between the university of', 'impact of the covid pandemic on medical education', 'covid and the university of british columbia undergraduate program a joint', 'covid and the university of british columbia undergraduate program', 'covid and medical student response team a joint response to the pandemic', 'a covid medical student response team a nimble organizational structure for the university', 'covid and medical student response team a joint response to the pandemic in bri', 'the covid pandemic and the medical student response team', 'an integrated medical student response team to address the covid pandemic in the university of ', 'an integrated medical student response team to address the covid pandemic']\n",
      "ref--------------------\n",
      "['transforming disruption into innovation a partnership between the covid medical student response team and the university of british columbia']\n",
      "doc--------------------\n",
      "['the covid pandemic caused substantial disruptions in medical education the university of british columbia ubc md undergraduate program mdup is the sixth largest medical school in north america mdup students and faculty developed a joint response to these disruptions to address the curriculum and public health challenges that the pandemic posed after clinical activities were suspended in march third and fourth year mdup students formed a covid medical student response team msrt to support frontline physicians public health agencies and community members affected by the pandemic a nimble organizational structure was developed across ubc campuses to ensure a rapid response to meet physician and community needs support from the faculty ensured the activities were safe for the public patients and students and facilitated the provision of curricular credit for volunteer activities meeting academic criteria as of june more than medical students had signed up to participate in projects the majority of students participated in projects supporting the health care system including performing contact tracing staffing public covid call centers distributing personal protective equipment and creating educational multimedia products many initiatives have been integrated into the mdup curriculum as scholarly activities or paraclinical electives for which academic credit is awarded this was made possible by the inherent flexibility of the mdup curriculum and a strong existing partnership between students and faculty through this process medical students were able to develop fundamental leadership advocacy communication and collaboration skills essential competencies for graduating physicians in developing a transparent accountable and inclusive organization students were able to effectively meet community needs during a crisis and create a sustainable and democratic structure capable of responding to future emergencies open dialogue between the msrt and the faculty allowed for collaborative problem solving and the opportunity to transform disruption into academic innovation ']\n"
     ]
    }
   ],
   "source": [
    "show_a_piece_of_data(generator,generator_tokenizer,test_data_txt[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a3c8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReRanker(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load scorer_tokenizer\n",
    "scorer_name = 'roberta-base'\n",
    "pt_model_name = 'saved_models/scorer_10.bin'\n",
    "scorer_tokenizer = RobertaTokenizer.from_pretrained(scorer_name)\n",
    "scorer = model.ReRanker(scorer_name, scorer_tokenizer.pad_token_id)\n",
    "scorer.load_state_dict(torch.load(pt_model_name,map_location=torch.device(device)))\n",
    "scorer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c21f5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_without_SimCLS(generator, test_data_txt, cand_num, show_results):\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    doc_txt = test_data_txt['abstract']\n",
    "    doc_ids = generator_tokenizer.batch_encode_plus(doc_txt, max_length = 512, pad_to_max_length=True)['input_ids']\n",
    "    doc_ids = to_tensor(doc_ids)\n",
    "    doc_input_mask = doc_ids != scorer_tokenizer.pad_token_id\n",
    "    doc_out = scorer.encoder(doc_ids, attention_mask=doc_input_mask)['last_hidden_state']\n",
    "    doc_emb = torch.mean(doc_out,dim = 1) # average over all word embeddings\n",
    "    \n",
    "    ref_txt = test_data_txt['title']\n",
    "    ref_ids = generator_tokenizer.batch_encode_plus(ref_txt, max_length = 512, pad_to_max_length=True)['input_ids']\n",
    "    ref_ids = to_tensor(ref_ids)\n",
    "    ref_input_mask = ref_ids != scorer_tokenizer.pad_token_id\n",
    "    ref_out = scorer.encoder(ref_ids, attention_mask=ref_input_mask)['last_hidden_state']\n",
    "    ref_emb = torch.mean(ref_out,dim = 1) \n",
    "    \n",
    "    cand_id = generator.generate(doc_ids,num_beams = 16,\n",
    "                                     #no_repeat_ngram_size=2,\n",
    "                                     diversity_penalty=1.0,\n",
    "                                     max_length = 20,\n",
    "                                     num_beam_groups = cand_num,\n",
    "                                     num_return_sequences = 1)\n",
    "    cands_txt = generator_tokenizer.batch_decode(cand_id, skip_special_tokens=True)\n",
    "    candidate_id = cand_id.view(-1, cand_id.size(-1))\n",
    "    cand_input_mask = candidate_id != scorer_tokenizer.pad_token_id\n",
    "    cand_out = scorer.encoder(candidate_id, attention_mask=cand_input_mask)['last_hidden_state'] \n",
    "    candidate_embs = torch.mean(cand_out,dim = 1)\n",
    "    \n",
    "    cand_similarity_score = torch.cosine_similarity(candidate_embs, doc_emb, dim=-1).item()\n",
    "    ref_similarity_score = torch.cosine_similarity(ref_emb, doc_emb, dim=-1).item()\n",
    "    \n",
    "    cands_rouge_scores = rouge_scorer.score(cands_txt[0],ref_txt[0])\n",
    "    rouge1_scores = cands_rouge_scores['rouge1'].fmeasure\n",
    "    rouge2_scores = cands_rouge_scores['rouge2'].fmeasure\n",
    "    rougeL_scores = cands_rouge_scores['rougeLsum'].fmeasure\n",
    "    \n",
    "    if show_results:\n",
    "        print('doc'+'-'*50)\n",
    "        print(doc_txt)\n",
    "        print('ref'+'-'*50)\n",
    "        print(ref_txt)\n",
    "        print('cand'+'-'*49)\n",
    "        print(cands_txt)\n",
    "        print('scores:'+'-'*50)\n",
    "        print(f'rouge1: {rouge1_scores}, rouge2: {rouge2_scores}, rougeL: {rougeL_scores}')\n",
    "        print(f'cand similarity: {cand_similarity_score}, ref similarity: {ref_similarity_score}')\n",
    "    regular_scores = {'rouge1': rouge1_scores, \n",
    "                    'rouge2': rouge2_scores,\n",
    "                    'rougeL': rougeL_scores,\n",
    "                    'similar': cand_similarity_score,\n",
    "                    'ref_similar':ref_similarity_score}\n",
    "    \n",
    "    return regular_scores,cands_txt\n",
    "\n",
    "def evaluate_SimCLS(generator, generator_tokenizer, scorer, scorer_tokenizer, \n",
    "                    test_data_txt, cand_num, show_piece_of_data):\n",
    "    # generate batch data\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    # 1, encode doc\n",
    "    doc_txt = test_data_txt['abstract']\n",
    "    doc_ids = generator_tokenizer.batch_encode_plus(doc_txt, max_length = 512, pad_to_max_length=True)['input_ids']\n",
    "    doc_ids = to_tensor(doc_ids)\n",
    "    # 2, encode true sum\n",
    "    ref_txt = test_data_txt['title']\n",
    "    ref_ids = generator_tokenizer.batch_encode_plus(ref_txt, max_length = 512, pad_to_max_length=True)['input_ids']\n",
    "    ref_ids = to_tensor(ref_ids)\n",
    "    # 3, generate cands\n",
    "    cands_ids = generator.generate(doc_ids,num_beams = cand_num,\n",
    "                                     #no_repeat_ngram_size=2,\n",
    "                                     diversity_penalty=1.0,\n",
    "                                     max_length = 20,\n",
    "                                     num_beam_groups = cand_num,\n",
    "                                     num_return_sequences = cand_num)\n",
    "    cands_txt = generator_tokenizer.batch_decode(cands_ids, skip_special_tokens=True)\n",
    "    # 4, get sentence embeddings\n",
    "        # doc emb\n",
    "    doc_input_mask = doc_ids != scorer_tokenizer.pad_token_id\n",
    "    doc_out = scorer.encoder(doc_ids, attention_mask=doc_input_mask)['last_hidden_state']\n",
    "    doc_emb = torch.mean(doc_out,dim = 1) # average over all word embeddings\n",
    "        # cands emb\n",
    "    candidate_id = cands_ids.view(-1, cands_ids.size(-1))\n",
    "    cand_input_mask = candidate_id != scorer_tokenizer.pad_token_id\n",
    "    cand_out = scorer.encoder(candidate_id, attention_mask=cand_input_mask)['last_hidden_state'] \n",
    "    candidate_embs = torch.mean(cand_out,dim = 1)\n",
    "        # ref emb\n",
    "    ref_input_mask = ref_ids != scorer_tokenizer.pad_token_id\n",
    "    ref_out = scorer.encoder(ref_ids, attention_mask=ref_input_mask)['last_hidden_state']\n",
    "    ref_emb = torch.mean(ref_out,dim = 1) \n",
    "    \n",
    "    similarity_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    for i in range(cand_num):\n",
    "        score = torch.cosine_similarity(candidate_embs[i], doc_emb, dim=-1).item()\n",
    "        similarity_scores.append(score)\n",
    "        cands_rouge_scores = rouge_scorer.score(cands_txt[i],ref_txt[0])\n",
    "        rouge1_scores.append(cands_rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(cands_rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(cands_rouge_scores['rougeLsum'].fmeasure)\n",
    "        \n",
    "    ref_similarity_score = torch.cosine_similarity(ref_emb, doc_emb, dim=-1).item()\n",
    "\n",
    "    if show_piece_of_data:\n",
    "        print('-'*50)\n",
    "        show_a_piece_of_data(generator, generator_tokenizer, test_data_txt)\n",
    "    max_index = similarity_scores.index(max(similarity_scores))\n",
    "    top1_scores = {'rouge1': rouge1_scores[max_index], \n",
    "                    'rouge2': rouge2_scores[max_index],\n",
    "                    'rougeL': rougeL_scores[max_index],\n",
    "                    'similar': similarity_scores[max_index],\n",
    "                    'ref_similar': ref_similarity_score}\n",
    "    \n",
    "    return top1_scores, cands_txt, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2250ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working sample: 113, time used last sample: 6.1196\r",
      "doc--------------------------------------------------\n",
      "health care providers have an ethical obligation to reduce suffering during a patient's end of life eol but few receive formal education on eol care principles the objective of this project was to determine the feasibility and potential benefits of an education initiative in which the principles of eol care were taught to senior level nursing students and practicing nurses to assess feasibility data regarding recruitment rates retention rates and implementation issues were collected workshop effectiveness was evaluated through use of the end of life nursing education consortium knowledge assessment test survey which evaluates knowledge levels regarding eol care principles a mixed effects linear model was used to test for changes from the preworkshop to postworkshop scores demographic information and satisfaction data were also collected nineteen students and nurses participated total n there was a statistically significant time difference p with the postworkshop scores being higher versus the preworkshop scores however no statistically significant workshop date difference p emerged satisfaction data were positive retention for the second workshop was negatively affected by covid the unique needs of patients nearing their eol are significant this project describes the implementation and outcomes of an education initiative focused on eol care principles that was both feasible and beneficial \n",
      "ref--------------------------------------------------\n",
      "facing the inevitable preparing nurses to deliver end of life care\n",
      "no SimCLS--------------------------------------------------\n",
      "implementation and outcomes of an education initiative on end of life care principles \n",
      " rouge:(0.33333333333333337, 0.2727272727272727, 0.33333333333333337)\n",
      "with SimCLS--------------------------------------------------\n",
      "end of life nursing education initiative for eol care \n",
      " rouge:(0.5, 0.22222222222222224, 0.39999999999999997)\n"
     ]
    }
   ],
   "source": [
    "# evaluate loop \n",
    "rouge1_noSimCLS = []\n",
    "rouge2_noSimCLS = []\n",
    "rougeL_noSimCLS = []\n",
    "\n",
    "rouge1_SimCLS = []\n",
    "rouge2_SimCLS = []\n",
    "rougeL_SimCLS = []\n",
    "\n",
    "references = []\n",
    "regular_cand_pred = []\n",
    "SimCLS_cand_pred = []\n",
    "\n",
    "num = 112\n",
    "for i in range(num,num+1):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    regular_scores,cand_txt = evaluate_without_SimCLS(\n",
    "        generator, test_data_txt[i:i+1], 16, False)\n",
    "    \n",
    "    rouge1_noSimCLS.append(regular_scores['rouge1'])\n",
    "    rouge2_noSimCLS.append(regular_scores['rouge2'])\n",
    "    rougeL_noSimCLS.append(regular_scores['rougeL'])\n",
    "    \n",
    "    \n",
    "    SimCLS_sores, cands_txt, top1_index = evaluate_SimCLS(generator, generator_tokenizer, scorer, \n",
    "                    scorer_tokenizer, test_data_txt[i:i+1], 16, False)\n",
    "    \n",
    "    rouge1_SimCLS.append(SimCLS_sores['rouge1'])\n",
    "    rouge2_SimCLS.append(SimCLS_sores['rouge2'])\n",
    "    rougeL_SimCLS.append(SimCLS_sores['rougeL'])\n",
    "    \n",
    "    references.append(test_data_txt[i]['title'])\n",
    "    regular_cand_pred.append(cand_txt[0])\n",
    "    SimCLS_cand_pred.append(cands_txt[top1_index])\n",
    "    time_end = time.time()\n",
    "    \n",
    "    time_used = time_end - time_start\n",
    "    print(f'current working sample: {i+1}, time used last sample: {round(time_used,4)}', end = '\\r')\n",
    "    \n",
    "print('doc'+ '-'*50)\n",
    "print(test_data_txt[num]['abstract'])\n",
    "print('ref'+ '-'*50)\n",
    "print(f'{references[0]}')\n",
    "print('no SimCLS' + '-'*50)\n",
    "print(f'{regular_cand_pred[0]} \\n rouge:{rouge1_noSimCLS[0], rouge2_noSimCLS[0],rougeL_noSimCLS[0]}')\n",
    "print('with SimCLS' + '-'*50)\n",
    "print(f'{SimCLS_cand_pred[0]} \\n rouge:{rouge1_SimCLS[0], rouge2_SimCLS[0],rougeL_SimCLS[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1e6da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SimCLS ROUGE1: 0.4528457018408491, ROUGE2: 0.240739539283891, ROUGEL: 0.3880134130034644\n",
      "After SimCLS ROUGE1: 0.4063123656705362, ROUGE2: 0.19135385057119345, ROUGEL: 0.33850068587817395\n"
     ]
    }
   ],
   "source": [
    "print(f'Before SimCLS ROUGE1: {np.mean(rouge1_noSimCLS)}, ROUGE2: {np.mean(rouge2_noSimCLS)}, ROUGEL: {np.mean(rougeL_noSimCLS)}')\n",
    "print(f'After SimCLS ROUGE1: {np.mean(rouge1_SimCLS)}, ROUGE2: {np.mean(rouge2_SimCLS)}, ROUGEL: {np.mean(rougeL_SimCLS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01eb3565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before SimCLS</th>\n",
       "      <th>After SimCLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rouge1</th>\n",
       "      <td>0.452846</td>\n",
       "      <td>0.406312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge2</th>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.191354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RougeL</th>\n",
       "      <td>0.388013</td>\n",
       "      <td>0.338501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Before SimCLS  After SimCLS\n",
       "Rouge1       0.452846      0.406312\n",
       "Rouge2       0.240740      0.191354\n",
       "RougeL       0.388013      0.338501"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.rename({0:'Rouge1', 1:'Rouge2', 2:'RougeL'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "20",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
